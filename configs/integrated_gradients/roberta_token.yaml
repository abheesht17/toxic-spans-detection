model_name: autotoken
name: bert
n_steps: 2
type: token
sample_index: 10 # should be in [0,1999] for test
out_dir: "/content/drive/My Drive/ToxicSpans/results/IG/bert_token"
word_out_file: ${out_dir}/word_importances
token_out_file: ${out_dir}/token_importances
viz_out_file: ${out_dir}/viz.html
ground_truths_file: ./data/tsd_test_spans.csv
results_dir: "/content/drive/My Drive/ToxicSpans/results/bert_token"
data_config:
  name: toxic_spans_tokens
  model_checkpoint_name: ${results_dir}/ckpts/checkpoint-2500
  train_files:
    validation: ./data/clean_trial.csv
  eval_files:
    test: ./data/tsd_test.csv
  label_cls: true
  cls_threshold: 0.3 # can be tuned
  token_threshold: 0.0 # can be tuned
  tokenizer_params:
    truncation: true
    max_length: 384 # originally we used 200
    padding: max_length
    return_offsets_mapping: true
pretrained_args:
  pretrained_model_name_or_path: ${data_config.model_checkpoint_name}
predictions_file: ${results_dir}/preds/spans-pred_original_test.txt
